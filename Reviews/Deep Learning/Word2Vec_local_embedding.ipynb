{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49819e1a-5f41-4e31-9a8d-9e53a613bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "using Random\n",
    "using Base.Iterators: partition\n",
    "import StatsBase.sample, StatsBase.Weights\n",
    "using Flux\n",
    "using Flux: onehot, onecold, onehotbatch\n",
    "using Flux: crossentropy, throttle, params\n",
    "using Zygote\n",
    "using BSON, JLD2, Statistics\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e8fd43-49a2-4e56-9f03-587a4769bf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining on GPU\n"
     ]
    }
   ],
   "source": [
    "use_cuda = true\n",
    "if use_cuda && CUDA.functional()\n",
    "    device = gpu\n",
    "    @info \"Training on GPU\"\n",
    "else\n",
    "    device = cpu\n",
    "    @info \"Training on CPU\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd61692f-115a-4aef-b2ec-aa6fdc38064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990278-element Vector{String}:\n",
       " \"I've taken a lot of spin classe\"\u001b[93m\u001b[1m ⋯ 768 bytes ⋯ \u001b[22m\u001b[39m\"'s kicking your butt in class!\"\n",
       " \"Family diner. Had the buffet. E\"\u001b[93m\u001b[1m ⋯ 279 bytes ⋯ \u001b[22m\u001b[39m\"ns. Next to the Clarion Hotel.\"\n",
       " \"Wow!  Yummy, different,  delici\"\u001b[93m\u001b[1m ⋯ 182 bytes ⋯ \u001b[22m\u001b[39m\"new!   You'll be glad you did!\"\n",
       " \"Cute interior and owner (?) gav\"\u001b[93m\u001b[1m ⋯ 473 bytes ⋯ \u001b[22m\u001b[39m\"ll try one of the draft wines.\"\n",
       " \"I am a long term frequent custo\"\u001b[93m\u001b[1m ⋯ 280 bytes ⋯ \u001b[22m\u001b[39m\". NEVER going back to dmitris!\"\n",
       " \"Loved this tour! I grabbed a gr\"\u001b[93m\u001b[1m ⋯ 743 bytes ⋯ \u001b[22m\u001b[39m\" my favorite parts of my trip!\"\n",
       " \"Amazingly amazing wings and hom\"\u001b[93m\u001b[1m ⋯ 131 bytes ⋯ \u001b[22m\u001b[39m\" checking out this hidden gem.\"\n",
       " \"This easter instead of going to\"\u001b[93m\u001b[1m ⋯ 465 bytes ⋯ \u001b[22m\u001b[39m\"ke they put a bit more effort.\"\n",
       " \"Had a party of 6 here for hibac\"\u001b[93m\u001b[1m ⋯ 463 bytes ⋯ \u001b[22m\u001b[39m\" the money I wouldn't go back.\"\n",
       " \"My experience with Shalimar was\"\u001b[93m\u001b[1m ⋯ 948 bytes ⋯ \u001b[22m\u001b[39m\"r such a wonderful experience.\"\n",
       " \"Locals recommended Milktooth, a\"\u001b[93m\u001b[1m ⋯ 57 bytes ⋯ \u001b[22m\u001b[39m\" the chance to experience this.\"\n",
       " \"Love going here for happy hour \"\u001b[93m\u001b[1m ⋯ 181 bytes ⋯ \u001b[22m\u001b[39m\"y the menu to suit your taste!\"\n",
       " \"Good food--loved the gnocchi wi\"\u001b[93m\u001b[1m ⋯ 114 bytes ⋯ \u001b[22m\u001b[39m\"ck, the food is just that good\"\n",
       " ⋮\n",
       " \"Great gym.  Was in Indy for 4 d\"\u001b[93m\u001b[1m ⋯ 327 bytes ⋯ \u001b[22m\u001b[39m\"ecked in.  Talk about service!\"\n",
       " \"This is a good pizza option - t\"\u001b[93m\u001b[1m ⋯ 730 bytes ⋯ \u001b[22m\u001b[39m\"nhouse Square area.  Doooo it.\"\n",
       " \"Don't misinterpret my 5-star re\"\u001b[93m\u001b[1m ⋯ 829 bytes ⋯ \u001b[22m\u001b[39m\") Wendy's I have ever been to.\"\n",
       " \"The front desk staff is very go\"\u001b[93m\u001b[1m ⋯ 390 bytes ⋯ \u001b[22m\u001b[39m\"o does not care about members.\"\n",
       " \"It is very rare for a restauran\"\u001b[93m\u001b[1m ⋯ 275 bytes ⋯ \u001b[22m\u001b[39m\" for me. Hats off to the chef.\"\n",
       " \"We redesigned my moms dress and\"\u001b[93m\u001b[1m ⋯ 148 bytes ⋯ \u001b[22m\u001b[39m\"in love every step of the way!\"\n",
       " \"Good, maybe very good.  I went\"\u001b[93m\u001b[1m ⋯ 2640 bytes ⋯ \u001b[22m\u001b[39m\"s are inexpensive.  It's BYOB.\"\n",
       " \"Latest addition to services fro\"\u001b[93m\u001b[1m ⋯ 261 bytes ⋯ \u001b[22m\u001b[39m\"the Boise area have Apple Pay?\"\n",
       " \"This spot offers a great, affor\"\u001b[93m\u001b[1m ⋯ 336 bytes ⋯ \u001b[22m\u001b[39m\"or the picnic table rest stop!\"\n",
       " \"This Home Depot won me over whe\"\u001b[93m\u001b[1m ⋯ 406 bytes ⋯ \u001b[22m\u001b[39m\"t it's neighboring competitor.\"\n",
       " \"For when I'm feeling like igno\"\u001b[93m\u001b[1m ⋯ 2257 bytes ⋯ \u001b[22m\u001b[39m\"ry. Calorie. Every single one.\"\n",
       " \"Located in the 'Walking Distri\"\u001b[93m\u001b[1m ⋯ 1138 bytes ⋯ \u001b[22m\u001b[39m\"lace in one word... overhyped.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "data_file = \"reviews_cleaner.csv\"\n",
    "df = CSV.read(data_file, DataFrame)\n",
    "texts = df.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655ad86-7868-4aef-aa15-26e6caab087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "function preprocess_text(text)\n",
    "    \n",
    "    text = lowercase(text)\n",
    "    text = replace(text, r\"[[:punct:]]+\" => \" \")\n",
    "    return split(text)\n",
    "end\n",
    "\n",
    "sentences = [preprocess_text(text) for text in texts]\n",
    "\n",
    "tokens = vcat(sentences...)\n",
    "alphabet = unique(tokens)\n",
    "\n",
    "freqs = Dict{String, Int}()\n",
    "for t in tokens\n",
    "    freqs[t] = get(freqs, t, 0) + 1\n",
    "end\n",
    "\n",
    "# Ensure \"UNK\" is always in the alphabet and freqs\n",
    "push!(alphabet, \"UNK\")\n",
    "freqs[\"UNK\"] = 0\n",
    "\n",
    "# Replace singleton tokens with an \"unknown\" marker\n",
    "for sentence in sentences\n",
    "    for i in 1:length(sentence)\n",
    "        if get(freqs, sentence[i], 0) == 1\n",
    "            sentence[i] = \"UNK\"\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "tokens = vcat(sentences...)\n",
    "alphabet = unique(tokens)\n",
    "\n",
    "# Subsampling\n",
    "train_alphabet = [word for word in alphabet if rand() < (1  - sqrt(1e-5/(freqs[word]/length(tokens))))]\n",
    "\n",
    "# Parse sentences to create context words\n",
    "function parse_sentences!(sentences, K, alphabet)\n",
    "    context_words = Dict(token => [] for token in alphabet)\n",
    "    for sentence in sentences\n",
    "        for (i, word) in enumerate(sentence)\n",
    "            word ∉ alphabet && continue\n",
    "            for j in i-K:i+K\n",
    "                (j < 1 || j > length(sentence) || i == j) && continue\n",
    "                context_word = sentence[j]\n",
    "                push!(context_words[word], context_word)\n",
    "                unique!(context_words[word])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    map(token -> isempty(context_words[token]) && delete!(context_words, token), alphabet)\n",
    "    filter!(token -> haskey(context_words, token) == true, alphabet)\n",
    "    return context_words\n",
    "end\n",
    "\n",
    "# Set the context window size\n",
    "window = 5\n",
    "context_words = parse_sentences!(sentences, window, alphabet)\n",
    "\n",
    "# Unigram sampler\n",
    "function unigram_sampler(alphabet, freqs, τ = 0.75)\n",
    "    sum_probs = sum(values(freqs).^τ)\n",
    "    return [freqs[token].^τ / sum_probs for token in alphabet]\n",
    "end\n",
    "\n",
    "token_weights = Weights(unigram_sampler(alphabet, freqs))\n",
    "\n",
    "# Define the model\n",
    "latent_dim = 300\n",
    "input_embedding = Dense(length(alphabet), latent_dim) |> device\n",
    "output_embedding = Dense(length(alphabet), latent_dim) |> device\n",
    "\n",
    "# Generate batch data for training\n",
    "function gen_batch(wordlist, ℓ = 1, K = 2*window)\n",
    "    data = []\n",
    "    for word in wordlist\n",
    "        for context_word_sample in rand(context_words[word], ℓ)\n",
    "            neg_samples = Vector{Int}(undef, K)\n",
    "            for i in 1:K\n",
    "                neg_samples[i] = sample(1:length(alphabet), token_weights)\n",
    "                while alphabet[neg_samples[i]] ∈ context_words[word]\n",
    "                    neg_samples[i] = sample(1:length(alphabet), token_weights)\n",
    "                end\n",
    "            end\n",
    "            push!(data, (onehot(word, alphabet), onehot(context_word_sample, alphabet), \n",
    "                    onehotbatch(alphabet[neg_samples], alphabet)))\n",
    "        end\n",
    "    end\n",
    "    data  \n",
    "end\n",
    "\n",
    "# Define the negative sampling loss function\n",
    "function negative_sampling_loss(word, context_word, neg_samples) \n",
    "     l_context = -log(σ(transpose(output_embedding(context_word)) * input_embedding(word)))  \n",
    "     l_negative = - sum(log.(σ.(transpose(-output_embedding(neg_samples))* input_embedding(word))))\n",
    "    return l_context + l_negative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aae4752-ea83-4827-8bc9-c7ba2f58c42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812-element Vector{SubString{String}}:\n",
       " \"me\"\n",
       " \"of\"\n",
       " \"being\"\n",
       " \"because\"\n",
       " \"my\"\n",
       " \"never\"\n",
       " \"got\"\n",
       " \"to\"\n",
       " \"see\"\n",
       " \"this\"\n",
       " \"place\"\n",
       " \"hope\"\n",
       " \"they\"\n",
       " ⋮\n",
       " \"pictures\"\n",
       " \"perspective\"\n",
       " \"cielo\"\n",
       " \"shot\"\n",
       " \"picture\"\n",
       " \"truck\"\n",
       " \"honestly\"\n",
       " \"looked\"\n",
       " \"hairs\"\n",
       " \"max\"\n",
       " \"single\"\n",
       " \"bummed\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03a16b4-d2f6-4f48-b361-ee9bfe58c56e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mBeginning training loop...\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 1\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 6.164083\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 2\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 3.2834537\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 3\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 4\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 3.0657735\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 5\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 6\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 7\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.9987671\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 8\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.9690604\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 9\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.8696418\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 10\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 11\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 12\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 13\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 14\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 15\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 16\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 17\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 18\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 19\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 20\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 21\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 22\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 23\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.81786\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 24\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 25\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 26\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.6143508\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 27\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 28\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 29\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 30\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 31\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 32\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 33\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.5658314\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 34\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.4650862\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 35\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 36\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 37\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.3871706\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 38\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 39\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 40\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 41\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.3035562\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 42\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 43\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 2.1990151\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 44\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 45\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 46\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 47\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 48\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 49\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.9032673\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 50\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 51\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 52\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 53\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 54\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.8338889\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 55\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 56\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 57\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.7362672\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 58\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.6928678\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 59\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 60\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 61\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.5875301\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 62\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 63\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 64\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 65\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.4073719\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 66\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 67\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 68\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 69\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 70\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 71\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 72\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 73\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.2286171\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 74\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 75\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 76\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 77\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 78\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.1747767\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 79\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 80\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 81\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 82\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 83\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.1287658\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 84\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 85\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 86\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 1.0129449\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 87\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 88\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 89\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 0.9899219\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 90\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 91\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 92\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 93\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 0.9838163\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 94\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 95\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 0.9386607\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 96\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 97\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 98\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 99\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mnew best embedding!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mloss = 0.67111164\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 64\n",
    "neg_sampling = 2 * window\n",
    "epochs = 100\n",
    "opt = ADAM(0.003)\n",
    "\n",
    "# Create data loader\n",
    "w2v_data = Flux.DataLoader(train_alphabet, batchsize=batch_size, shuffle=true)\n",
    "\n",
    "# Training loop\n",
    "@info(\"Beginning training loop...\")\n",
    "loss = Inf\n",
    "last_improvement = 0\n",
    "for epoch in 1:epochs\n",
    "    @info \"Epoch $epoch\"\n",
    "    for word_batch in w2v_data\n",
    "        data = gen_batch(word_batch)\n",
    "        ps = params(input_embedding, output_embedding)\n",
    "        _, back = Zygote.pullback(ps) do\n",
    "            losses = [negative_sampling_loss(dt[1], dt[2], dt[3]) for dt in data]\n",
    "            sum(losses) / length(losses)\n",
    "        end\n",
    "        grads = back(1f0)\n",
    "        Flux.Optimise.update!(opt, ps, grads)\n",
    "    end\n",
    "    new_loss = begin\n",
    "        losses = [negative_sampling_loss(dt[1], dt[2], dt[3]) for dt in gen_batch(rand(collect(w2v_data)))]\n",
    "        sum(losses) / length(losses)\n",
    "    end\n",
    "    if new_loss < loss\n",
    "        loss = new_loss\n",
    "        @info \"new best embedding!\"\n",
    "        @info(\"loss = $loss\")\n",
    "        model_params = cpu.(params(input_embedding, output_embedding))\n",
    "        BSON.@save \"word2vec.bson\" model_params\n",
    "        jldsave(\"word2vec.jld2\"; input_embedding=cpu.(params(input_embedding)), \n",
    "                                output_embedding=cpu.(params(output_embedding)))\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement ≥ 15\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "444c3f3f-bb36-4720-82eb-79c939c411ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{String}:\n",
       " \"disrespectful bouncer accuses d\"\u001b[93m\u001b[1m ⋯ 40 bytes ⋯ \u001b[22m\u001b[39m\" bankrupt attitude keep pushing\"\n",
       " \"ralphs park nice restaurant don\"\u001b[93m\u001b[1m ⋯ 287 bytes ⋯ \u001b[22m\u001b[39m\"d hire cook brennan family ugh\"\n",
       " \"cute spot friendly awesome serv\"\u001b[93m\u001b[1m ⋯ 27 bytes ⋯ \u001b[22m\u001b[39m\"licious happened upon spot glad\"\n",
       " \"time good food wish dine right come back whole\"\n",
       " \"nice spot sobro love tucked awa\"\u001b[93m\u001b[1m ⋯ 149 bytes ⋯ \u001b[22m\u001b[39m\"rvation walk away feeling full\"\n",
       " \"served rude lady burped face di\"\u001b[93m\u001b[1m ⋯ 28 bytes ⋯ \u001b[22m\u001b[39m\"eeded anything else walked away\"\n",
       " \"dr youngblood amazing he best d\"\u001b[93m\u001b[1m ⋯ 44 bytes ⋯ \u001b[22m\u001b[39m\"e office admin really cool well\"\n",
       " \"ordered pizza take couple time \"\u001b[93m\u001b[1m ⋯ 38 bytes ⋯ \u001b[22m\u001b[39m\"ling day good day burnt cheesed\"\n",
       " \"view food great usually get sea\"\u001b[93m\u001b[1m ⋯ 149 bytes ⋯ \u001b[22m\u001b[39m\"ays attentive friendly service\"\n",
       " \"last night girlfriend experienc\"\u001b[93m\u001b[1m ⋯ 781 bytes ⋯ \u001b[22m\u001b[39m\"sible would give 0 star rating\"\n",
       " \"love place matter get delicious\"\u001b[93m\u001b[1m ⋯ 136 bytes ⋯ \u001b[22m\u001b[39m\"n sandwich definitely must try\"\n",
       " \"great addition clearwater excel\"\u001b[93m\u001b[1m ⋯ 170 bytes ⋯ \u001b[22m\u001b[39m\"z con gandules ever added menu\"\n",
       " \"selected 1 star required otherw\"\u001b[93m\u001b[1m ⋯ 724 bytes ⋯ \u001b[22m\u001b[39m\"t salad lobster mac cheese bad\"\n",
       " ⋮\n",
       " \"got pizza delivery memorial day\"\u001b[93m\u001b[1m ⋯ 613 bytes ⋯ \u001b[22m\u001b[39m\"experience even remotely worth\"\n",
       " \"pal summer special two menu ite\"\u001b[93m\u001b[1m ⋯ 99 bytes ⋯ \u001b[22m\u001b[39m\" entree recall happy everything\"\n",
       " \"absolutely delicious 33 turista\"\u001b[93m\u001b[1m ⋯ 756 bytes ⋯ \u001b[22m\u001b[39m\"ck cab ride anyone center city\"\n",
       " \"loved cocktail whiskey business\"\u001b[93m\u001b[1m ⋯ 46 bytes ⋯ \u001b[22m\u001b[39m\"e 3 valet parking complimentary\"\n",
       " \"came sunday made right closing \"\u001b[93m\u001b[1m ⋯ 366 bytes ⋯ \u001b[22m\u001b[39m\"se historical perspective well\"\n",
       " \"proposed beautiful girlfriend a\"\u001b[93m\u001b[1m ⋯ 146 bytes ⋯ \u001b[22m\u001b[39m\"eople cielo making night great\"\n",
       " \"nothing wowed good beer selecti\"\u001b[93m\u001b[1m ⋯ 89 bytes ⋯ \u001b[22m\u001b[39m\"nice guess id give another shot\"\n",
       " \"ok 7 review 5 star suspicious \"\u001b[93m\u001b[1m ⋯ 1125 bytes ⋯ \u001b[22m\u001b[39m\"side thank max incredible work\"\n",
       " \"food awesome hot fresh hand bes\"\u001b[93m\u001b[1m ⋯ 43 bytes ⋯ \u001b[22m\u001b[39m\" show food bagging overall back\"\n",
       " \"went breakfast last week food s\"\u001b[93m\u001b[1m ⋯ 134 bytes ⋯ \u001b[22m\u001b[39m\"ake good also highly recommend\"\n",
       " \"world people dunkin donut venti\"\u001b[93m\u001b[1m ⋯ 316 bytes ⋯ \u001b[22m\u001b[39m\"rse one hero among stand chose\"\n",
       " \"im bummed bro soooo bummed look\"\u001b[93m\u001b[1m ⋯ 514 bytes ⋯ \u001b[22m\u001b[39m\"ext time sure ill take picture\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b05fbd80-34e5-4f2e-8339-9409001c5920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing"
     ]
    }
   ],
   "source": [
    "print(get_embedding(\"disrespectful\", alphabet, input_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e6e5516-bd1b-4576-b195-bad7078e64ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disrespectful\n",
      "bouncer\n",
      "accuses\n",
      "drunk\n",
      "cologne\n",
      "never\n",
      "got\n",
      "see\n",
      "place\n",
      "hope\n",
      "go\n",
      "bankrupt\n",
      "attitude\n",
      "keep\n",
      "pushing\n"
     ]
    }
   ],
   "source": [
    "for word in split(df.text_clean[1])\n",
    "    print(word)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b0eba-93ac-4a99-b2b8-a3733a5bc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function preprocess_text(text)\n",
    "    # Convert to lowercase\n",
    "    text = lowercase(text)\n",
    "    # Remove punctuation and handle multiple punctuation\n",
    "    text = replace(text, r\"[[:punct:]]+\" => \" \")\n",
    "    # Split into words (tokens)\n",
    "    return split(text)\n",
    "end\n",
    "\n",
    "# Apply preprocessing\n",
    "df.text_clean = preprocess_text.(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42378dbd-8b60-4d42-a18b-ca104f787d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"G:/LLM/own.csv\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained embeddings\n",
    "latent_dim = 300\n",
    "input_embedding = Dense(length(alphabet), latent_dim)\n",
    "\n",
    "model_params = JLD2.load(\"word2vec.jld2\")\n",
    "Flux.loadparams!(input_embedding, model_params[\"input_embedding\"])\n",
    "\n",
    "# Function to get the embedding for a word\n",
    "function get_embedding(word, alphabet, input_embedding)\n",
    "    if word in alphabet\n",
    "        return input_embedding(onehot(word, alphabet))\n",
    "    else\n",
    "        return nothing\n",
    "    end\n",
    "end\n",
    "\n",
    "# Calculate mean embedding for each review\n",
    "function mean_embedding(review, alphabet, input_embedding)\n",
    "    embeddings = [get_embedding(word, alphabet, input_embedding) for word in split(review) if get_embedding(word, alphabet, input_embedding) !== nothing]\n",
    "    if length(embeddings) == 0\n",
    "        return fill(0.0, latent_dim)\n",
    "    else\n",
    "        return mean(hcat(embeddings...), dims=2)[:]\n",
    "    end\n",
    "end\n",
    "\n",
    "# Calculate mean vector for each review\n",
    "df.mean_vector = mean_embedding.(df.text_clean, Ref(alphabet), Ref(input_embedding))\n",
    "\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "CSV.write(\"G:/LLM/own60k.csv\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6feb8db-0eab-4481-bff2-36f43a411c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
